# AnomHome Overmind Configuration
# Copy this file to .env and fill in your values

# ====================================
# AI Provider Configuration  
# ====================================
# Set to 'local' to use JugiAI (llama-server CMake-built)
# Set to 'openai' to use OpenAI API
AI_PROVIDER=local

# ====================================
# JugiAI Configuration (Local llama-server)
# ====================================
# Port where your llama-server is running
# Start llama-server with: ~/llama.cpp/build/llama-server -m /path/to/model.gguf --port 8080 --host localhost
LOCAL_SERVER_PORT=8080

# Optional: Model path (for reference, llama-server loads the model itself)
# LOCAL_MODEL_PATH=/path/to/your/model.gguf

# ====================================
# OpenAI Configuration (if using OpenAI)
# ====================================
# OPENAI_API_KEY=sk-your-openai-api-key-here

# ====================================
# Server Configuration
# ====================================
PORT=3000
HOST=0.0.0.0

# ====================================
# Feature Toggles
# ====================================
ENABLE_OPENAI=true
ENABLE_LINKS=true
ENABLE_UPLOADS=true
ENABLE_FILES=true
ENABLE_CAMERAS=true
ENABLE_NOTES=true

# ====================================
# File Configuration
# ====================================
# Root directory for file browser (defaults to home directory)
FILE_BROWSER_ROOT=

# Upload Configuration
# Maximum file size in MB (default: 100)
MAX_UPLOAD_SIZE=100
MAX_FILE_SIZE_MB=50

# ====================================
# Security
# ====================================
# Set a secret key for session management
SECRET_KEY=your_secret_key_here

# ====================================
# JugiAI Setup Instructions
# ====================================
# 1. Build llama.cpp with CMake:
#    cd ~/llama.cpp
#    mkdir build && cd build
#    cmake ..
#    cmake --build . --config Release
#
# 2. Download a GGUF model (e.g., from Hugging Face)
#    
# 3. Start llama-server for JugiAI:
#    ~/llama.cpp/build/llama-server \
#      -m /path/to/your/model.gguf \
#      --port 8080 \
#      --host localhost \
#      --ctx-size 4096 \
#      --n-gpu-layers 32
#
# 4. Test JugiAI connection:
#    node test-llama-server.js 8080
#
# 5. Start Overmind:
#    npm start
